---
layout: post
title: 중고나라 게시글 데이터를 크롤링하는 방법
categories: dailynote
tags: crawling database
---

# 막연히 든 생각

우선 막연히 떠오른 생각은 그냥 게시글 하나의 html소스를 통째로 긁어오면 되지 않을까 하는 것이었다.
하지만 2024년 현재 중고나라는 url에서 인덱싱을 하고 있지 않았고, 그 어떤 게시물을 들어가도 url이 바뀌지 않았다.
그리고 게시글에서 새로고침을 하면 카페 첫 대문으로 되돌아오고 이 상황에서 뒤로가기를 눌러도 게시글로 돌아가지도 않았다.

그래서 인프라를 분석해보기로 마음먹었다.

기본적으로 웹페이지 렌더링은 크게 두 가지 범주로, 그리고 한 범주 내에서 또 4가지로 구분된다

- **Static Web Page** : 단순하게 웹서버에 이미 저장된 문서를 클라이언트에 전송함 (중간에 변화 없음, 기능 제한적)
- **Dynamic Web Page** : 웹서버에 저장된 자원을 이용해 조건에 따라 만들어 전송함
    - **Client Side Rendering** : 데이터가 없는 html문서나 static 파일만 받아와 먼저 로드하고 이후에 데이터를 요청하여 받아오는 방식, 
    모든 로직은 서버가 아닌 클라이언트 측에서 처리
    - **Server Side Rendering** : CSR과는 반대로 서버에서 동적으로 html문서에 데이터까지 전부 삽입하여 html을 반환한다
    - **Multi Page Application** : 새로운 페이지를 요청할 때마다 정적 리소스가 다운로드되고, 그에 맞춰 전체 페이지를 다시 렌더링하는 방식(SSR방식으로)
      인터넷 주소창에 주소를 입력하거나 링크를 클릭하는 등의 사용자가 어떠한 요청을 하게 되면, 그에 맞는 완전한 페이지를 받아오고 다시 렌더링된다.
    - **Single Page Application** : 웹 애플리케이션에 필요한 모든 정적 리소스를 최초 한 번만 다운로드를 한다.
        그 이후, 새로운 페이지에 대한 요청이 있을 때마다 페이지 갱신에 필요한 데이터만 전달 받고 그 정보를 기준으로 페이지를 갱신한다. (즉, CSR 방식으로 렌더링한다.)

이런 개념들을 가지고 인프라를 분석해 보자.

## 그럼 어떻게..?

크롤링은 Python 뿐만 아니라 정말 수많은 언어에서 외부 라이브러리 또는 모듈로 지원한다.

더 깊게 들어가면 크롤링, 스크래핑 등의 개념으로 분류할 수 있는데 이건 접어두고 필요한 것부터 알아보자

```javascript
const puppeteer = require('puppeteer');

(async () => {
    const browser = await puppeteer.launch();
    const page = await browser.newPage();
    
    await page.goto('https://crwlnoti.shop/discount/');
    // 이 url은 탐지하려면 웹사이트에 따라 바뀔 수 있음

    // 최초 HTML 소스 가져오기
    const initialHTML = await page.content();

    // 자바스크립트가 로드된 후 HTML 소스 가져오기
    await new Promise(resolve => setTimeout(resolve, 3000));  // 대략적인 로드 시간
    const finalHTML = await page.content();

    if (initialHTML === finalHTML) {
        console.log('Server-Side Rendering (SSR) 또는 Multi-Page Application (MPA)');
    } else {
        console.log('Client-Side Rendering (CSR) 또는 Single-Page Application (SPA)');
    }

    await browser.close();
})();
```

```text
출력 : Client-Side Rendering (CSR) 또는 Single-Page Application (SPA)
```

자바스크립트의 puppeteer 라이브러리를 이용해 CSR과 SSR을 구분하는 코드 스니펫이다.
초기 소스와 시간이 더 지나 완전히 로드가 완료된 소스를 비교하여 구분하는 것이다.

크롤노티, 플리맨과 같은 개인이 유지보수하는 웹사이트들은 대부분 react가 사용된다. react는 보통 SPA 방식으로 알려져 있다.
클라이언트에는 정적인 부분만 보내고, 사용자가 이동하면서 바뀌는 부분만 데이터를 보내서 갱신해 웹서버의 부담을 줄이는 것이다.

중고나라는 SSR, MPA로 출력이 나온다. 이동할때마다 화면이 깜빡거린다. MPA의 특징이다.
새로운 페이지를 요청할 때마다 정적 리소스가 다운로드되고, 이에 맞춰서 전체가 다시 렌더링된다.

물론 이것도 완전히 discrete하게 분류할 수는 없다. 중고나라는 왼쪽의 사이드바가 안 깜빡거리고
중간의 게시글과 게시판만 깜빡거린다.

플리맨, 크롤노티도 거의 마찬가지이다. 플리맨은 처음 접속할 때 로딩시간이 길다.
하지만 게시글을 볼 때, 블럭이 새로 로딩될 때는 깜빡임이 없다. 리액트의 특징이다. 이건 개발자도 명시한 부분이다.
그러넫 크롤노티는 잘 모르겠다. 깜빡거리는 부분과 아닌 부분이 혼재한다.

## 그럼 대체 크롤링은 언제..?

지금 바로 해볼 것이다.

{% gist 36e04751b62fc7b2af0219491970a4d4 %}

실패한 과정이 많지만 다 쓰기가 귀찮아서.. 그냥 성공한 코드만 쓴다.
이건 중고나라의 10050146번 게시글의 소스를 가져오는 코드이다.
일단 대부분의 사람들이 selenium으로 각종 태그로 들어가서 필요한 데이터만 뽑아오는데 공을 들인다.
하지만 이러면 시간이 오래 걸릴 뿐더러 뽑아낼 데이터가 방대하면 코드도 복잡해진다.
그래서 나는 그냥 중고나라 게시글 데이터가 들어있는 html소스만 통으로 가져오기로 했다.
어차피 파싱은 나중에 로컬에서 하면 되기 때문에.. 다른 언어를 사용할 수도 있고 말이다.

실패한 과정은 대충 이렇다.
```text
url에서 게시글을 구분할 수 없어서 개발자 도구로 네트워크 분석 -> 
타입을 정렬해서 document가 있는지 찾아봄 ->
정말 다행히도 ArticleRead.nhn 도메인에서 게시글 내용만 따로 불러오는 거였다. -> 
근데 이걸 또 iframe에다 뿌려줘서 iframe으로 한단계 더 들어가서 소스를 다운로드 해야 했다.
```

그나마 다행인 점은 크롤링 봇을 따로 막지 않았다는 점.. 하긴 뭐 네이버에서 애지중지할 대단한 데이터도 아니고
그것도 제3자가 운영하는 카페에서 개인들이 막 올렸다 지웠다 그리고 장사꾼들 글 올라오는 곳인데 굳이 막을 이유는
없다고 생각한다.

이제 자야겠음 하암..